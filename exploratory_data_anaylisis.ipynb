{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfa15c4",
   "metadata": {},
   "source": [
    "# Eploratory Data Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d442a",
   "metadata": {},
   "source": [
    "### Beforehand, I should apologize. My code is ugly. I know and I am already taking care of my python skills. \n",
    "### But it works.\n",
    "### In the future I will probably fix it (all pep 8 included)\n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a581f4",
   "metadata": {},
   "source": [
    "## Lets clean a little more (while explorating a little)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next step after cleaning data is to READ THE DATA, study it, scrutinize it with the purpose of understanding what it can say and offer\n",
    "#This step is the EXPLORATORY Data Analysis\n",
    "\n",
    "#So, studying the cleaned data we can see that there are more things to clean. In some columns we can identify word that repeat more often than others and many others patterns.\n",
    "\n",
    "\n",
    "#In this case, Ken Jee identified and treated some things that are titled as a subtitle and cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f00dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "df=pd.read_csv(\"./KenJee_salary_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f971081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Job Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4cc60",
   "metadata": {},
   "source": [
    "## Job title and seniority  (Ken Jee idea but greatly improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9648a",
   "metadata": {},
   "source": [
    "### Analyzing Job titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf05fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#: there are words that are reapeted more often than others.\n",
    "\n",
    "#If we search for more extended types of data scientist, we will find just 5 or 6 (as Ken Jee done).\n",
    "# def title_simplifier(title):\n",
    "#     if 'data scientist' in title.lower():\n",
    "#         return 'data scientist'\n",
    "#     elif 'data engineer' in title.lower():\n",
    "#         return 'data engineer'\n",
    "#     elif 'analyst' in title.lower():\n",
    "#         return 'analyst'\n",
    "#     elif 'machine learning' in title.lower():\n",
    "#         return 'machine learning'\n",
    "#     elif 'manager' in title.lower():\n",
    "#         return 'manager'\n",
    "#     elif 'director' in title.lower():\n",
    "#         return 'director'\n",
    "#     else:\n",
    "#         return 'na'\n",
    "    \n",
    "#but, If we read data we have got here we can find many more data related job positions. ALERT! doing this we can make a wrong \n",
    "#play, taking into account that we can be overfittint our model to the data we have, it could be better apreciated if we compare\n",
    "#our results in this data with the results with the testing data or unseen data. If the results with other data is much more \n",
    "#different one of the reasons could be this overfitting\n",
    "def title_simplifier(title):\n",
    "    if 'data scientist' in title.lower():\n",
    "        return 'data scientist'\n",
    "    elif 'data science' in title.lower():\n",
    "        return 'data scientist'\n",
    "    elif 'data engineer' in title.lower():\n",
    "        return 'data engineer'\n",
    "    elif 'analyst' in title.lower():\n",
    "        return 'analyst'\n",
    "    elif 'machine learning' in title.lower():\n",
    "        return 'machine learning engineer'\n",
    "    elif 'manager' in title.lower():\n",
    "        return 'manager'\n",
    "    elif 'director' in title.lower():\n",
    "        return 'director'\n",
    "    elif 'scientist' in title.lower():\n",
    "        return 'other scientist'\n",
    "    elif 'data modeler' in title.lower():\n",
    "        return 'data modeler'\n",
    "    elif 'data' and 'anal' in title.lower():\n",
    "        return 'data analitics'\n",
    "    else:\n",
    "        return 'na'  #not applicable\n",
    "    \n",
    "    \n",
    "#FLAG: OVERFITTING???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ac686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "df['job_simp'] = df['Job Title'].apply(title_simplifier)\n",
    "df['job_simp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bcfbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #This cell is made to check those job_titles that are not contemplated in the \"title_simplication\" function clasification:\n",
    "        #I made a list with just the data that is categorized as \"na\" (not applicable) in \"simp_job\" column and analyze it.\n",
    "        #Finaly, I modified the \"title_simplification\" funtion if convenient.\n",
    "job_title_variation=df.apply(lambda x: x['Job Title'] if x['job_simp']==\"na\" else 1,axis=1)\n",
    "job_title_variation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9112d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc32d80f",
   "metadata": {},
   "source": [
    "### Analysing seniority:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0900263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#This function looks for the seniority asked in the title of each job asking (Job Title column)\n",
    "def seniority(title):\n",
    "    if 's.sr' in title.lower() or 's. sr' in title.lower() or 's sr' in title.lower() or 'ssr' in title.lower() or 's. senior' in title.lower() or 'semi or' in title.lower() or 'middle' in title.lower() or 'mid' in title.lower():\n",
    "        return 'ssr'\n",
    "    elif 'jr' in title.lower() or 'junior' in title.lower():\n",
    "        return 'jr'\n",
    "    elif 'sr' in title.lower() or 'senior' in title.lower() or 'lead' in title.lower() or 'principal' in title.lower():\n",
    "        return 'sr'\n",
    "    else:\n",
    "        return 'na'\n",
    "df['seniority_by_title'] = df['Job Title'].apply(seniority)\n",
    "df.seniority_by_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaed40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #This cell is made to check those job_titles that are not contemplated in the \"seniority\" function clasification:\n",
    "        #I made a list with just the data that is categorized as \"na\" (not applicable) in \"seniority\" column and analyze it.\n",
    "        #Finaly, I modified the \"seniority\" funtion if convenient.\n",
    "senior=df.apply(lambda x: x['Job Title'] if x['seniority']==\"na\" else 1,axis=1)\n",
    "senior.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#This function looks for the seniority asked in the description of each job asking (Job Description column)\n",
    "import re\n",
    "        \n",
    "\n",
    "def experience (job_description):\n",
    "\n",
    "    if 'years experience' in job_description.lower(): #find each position in 'Job Description' thas says \"years experience\" (years experience)\n",
    "        text_list = job_description.split('years experience')\n",
    "        text_list = [x[-15:] for x in text_list] #Just take last 15 characters before the searched string (where the text says how many years)\n",
    "        \n",
    "        if len(text_list)>1: #the next line only is executed if there is more than one item in the list\n",
    "            text_list = text_list[:-1] #delete last item in list (couse it does not contain a number)\n",
    "        \n",
    "        # Here I transform words to numbers\n",
    "        for index, item in enumerate(text_list):\n",
    "            if \"one\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('one','1')\n",
    "            if \"two\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('two','2')\n",
    "            if \"three\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('three','3')\n",
    "            if \"four\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('four','4')\n",
    "            if \"five\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('five','5')\n",
    "            if \"six\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('six','6')\n",
    "            if \"seven\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('seven','7')\n",
    "            if \"eight\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('eight','8')\n",
    "            if \"nine\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('nine','9')\n",
    "            if \"ten\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('ten','10')\n",
    "            if \"eleven\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('eleven','11')\n",
    "            if \"twelve\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('twelve','12')\n",
    "\n",
    "        # Here numbers are everthing left. (thanks stackoverflow)\n",
    "        pattern = re.compile(r'\\d+')\n",
    "        num_list = [\" \".join(pattern.findall(item)) for item in text_list] #Take each number and create a list of numbers.\n",
    "        #NOTE: when there are decimals numbers, they are taked as different numbers (i.e. 3.5 are taked as 3 and 5). But, taking into acount that the average between this and it correct value not affect much, it is irrelevant to correct it.\n",
    "\n",
    "        num_list = max(num_list)   #finally we take the bigger number\n",
    "        #(this means that if the compay ask for 2 years of experience with python and 5 years with ML, the important number is the 5)\n",
    "                \n",
    "        text = ' '.join(num_list)\n",
    "        #I choose to do it like this because if I join number I can't differenciate a decimal to decide which is larger('10 3' -> 103)\n",
    "        last_list = text.split(' ')\n",
    "        text = max(last_list)   #finally we take the bigger number\n",
    "        #(this means that if the compay ask for 2 years of experience with python and 5 years with ML, the important number is the 5)\n",
    "             \n",
    "        if text == '': #Writes 'na' when text is empty (because in the text before de searched string there is no numbers)\n",
    "            text = 'na'\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    elif 'years of' in job_description.lower(): #find each position in 'Job Description' thas says \"years of\" (years of experience)\n",
    "        text_list = job_description.split('years of')\n",
    "        text_list = [x[-15:] for x in text_list] #Just take last 15 characters before the searched string (where the text says how many years)\n",
    "        \n",
    "        if len(text_list)>1: #the next line only is executed if there is more than one item in the list\n",
    "            text_list = text_list[:-1] #delete last item in list (couse it does not contain a number)\n",
    "        \n",
    "        # Here I transform words to numbers\n",
    "        for index, item in enumerate(text_list):\n",
    "            if \"one\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('one','1')\n",
    "            if \"two\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('two','2')\n",
    "            if \"three\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('three','3')\n",
    "            if \"four\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('four','4')\n",
    "            if \"five\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('five','5')\n",
    "            if \"six\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('six','6')\n",
    "            if \"seven\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('seven','7')\n",
    "            if \"eight\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('eight','8')\n",
    "            if \"nine\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('nine','9')\n",
    "            if \"ten\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('ten','10')\n",
    "            if \"eleven\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('eleven','11')\n",
    "            if \"twelve\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('twelve','12')\n",
    "\n",
    "        # Here numbers are everthing left. (thanks stackoverflow)\n",
    "        pattern = re.compile(r'\\d+')\n",
    "        num_list = [\" \".join(pattern.findall(item)) for item in text_list] #Take each number and create a list of numbers.\n",
    "        #NOTE: when there are decimals numbers, they are taked as different numbers (i.e. 3.5 are taked as 3 and 5). But, taking into acount that the average between this and it correct value not affect much, it is irrelevant to correct it.\n",
    "\n",
    "        num_list = max(num_list)   #finally we take the bigger number\n",
    "        #(this means that if the compay ask for 2 years of experience with python and 5 years with ML, the important number is the 5)\n",
    "                \n",
    "        text = ' '.join(num_list)\n",
    "        #I choose to do it like this because if I join number I can't differenciate a decimal to decide which is larger('10 3' -> 103)\n",
    "        last_list = text.split(' ')\n",
    "        text = max(last_list)   #finally we take the bigger number\n",
    "        #(this means that if the compay ask for 2 years of experience with python and 5 years with ML, the important number is the 5)\n",
    "    \n",
    "        if text == '': #Writes 'na' when text is empty (because in the text before de searched string there is no numbers)\n",
    "            text = 'na'    \n",
    "    \n",
    "        return text\n",
    "    \n",
    "    elif 'years’' in job_description.lower(): #find each position in 'Job Description' thas says \"years’\" (year's experience)\n",
    "        text_list = job_description.split('years’')\n",
    "        text_list = [x[-15:] for x in text_list] #Just take last 15 characters before the searched string (where the text says how many years)\n",
    "        \n",
    "        if len(text_list)>1: #the next line only is executed if there is more than one item in the list\n",
    "            text_list = text_list[:-1] #delete last item in list (couse it does not contain a number)\n",
    "        \n",
    "        # Here I transform words to numbers\n",
    "        for index, item in enumerate(text_list):\n",
    "            if \"one\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('one','1')\n",
    "            if \"two\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('two','2')\n",
    "            if \"three\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('three','3')\n",
    "            if \"four\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('four','4')\n",
    "            if \"five\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('five','5')\n",
    "            if \"six\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('six','6')\n",
    "            if \"seven\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('seven','7')\n",
    "            if \"eight\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('eight','8')\n",
    "            if \"nine\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('nine','9')\n",
    "            if \"ten\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('ten','10')\n",
    "            if \"eleven\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('eleven','11')\n",
    "            if \"twelve\" in text_list[index].lower():\n",
    "                text_list[index] = text_list[index].lower().replace('twelve','12')\n",
    "\n",
    "        # Here numbers are everthing left. (thanks stackoverflow)\n",
    "        pattern = re.compile(r'\\d+')\n",
    "        num_list = [\" \".join(pattern.findall(item)) for item in text_list] #Take each number and create a list of numbers.\n",
    "        #NOTE: when there are decimals numbers, they are taked as different numbers (i.e. 3.5 are taked as 3 and 5). But, taking into acount that the average between this and it correct value not affect much, it is irrelevant to correct it.\n",
    "\n",
    "        num_list = max(num_list)   #finally we take the bigger number\n",
    "        #(this means that if the compay ask for 2 years of experience with python and 5 years with ML, the important number is the 5)\n",
    "                \n",
    "        text = ' '.join(num_list)\n",
    "        #I choose to do it like this because if I join number I can't differenciate a decimal to decide which is larger('10 3' -> 103)\n",
    "        last_list = text.split(' ')\n",
    "        text = max(last_list)   #finally we take the bigger number\n",
    "        #(this means that if the compay ask for 2 years of experience with python and 5 years with ML, the important number is the 5)\n",
    "\n",
    "        if text == '': #Writes 'na' when text is empty (because in the text before de searched string there is no numbers)\n",
    "            text = 'na'\n",
    "        \n",
    "        return text\n",
    "   \n",
    "    else:\n",
    "        return 'na'\n",
    "    \n",
    "    \n",
    "df['seniority_by_description'] = df['Job Description'].apply(experience)\n",
    "\n",
    "df['seniority_by_description'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Here I turn years of experience asked to seniority classification:\n",
    "    #characterizing seniority by: https://devetry.com/blog/junior-vs-mid-vs-senior-software-engineers-–-experience-skills-expectations/\n",
    "    \n",
    "def years_2_seniority(years_c):\n",
    "    if years_c.lower().isdigit():\n",
    "        if int(years_c.lower()) <= 2:\n",
    "            return 'jr'\n",
    "        elif int(years_c.lower()) > 2 and int(years_c.lower()) <= 5:\n",
    "            return 'ssr'\n",
    "        elif int(years_c.lower()) > 5:\n",
    "            return 'sr'\n",
    "    else:\n",
    "        return 'na'    \n",
    "    \n",
    "df['seniority_by_description'] = df['seniority_by_description'].apply(years_2_seniority)\n",
    "df['seniority_by_description'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Finally I made de last seniorities characterization ranking by seniority_by_title and then seniority_by_description\n",
    "df['seniority'] = df.apply(lambda x: x['seniority_by_description'] if 'na' in x['seniority_by_title'] else x['seniority_by_title'],axis=1)\n",
    "df['seniority'].value_counts()\n",
    "\n",
    "#job_title_variation=df.apply(lambda x: x['Job Title'] if x['job_simp']==\"na\" else 1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f939777",
   "metadata": {},
   "source": [
    "## Checking job_state column  (Ken Jee idea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Ok, now gonna see how the values of job_state are repeated\n",
    "df['job_state'].value_counts()\n",
    "#Here can be seen \"Los Angeles\" (an error, obviously)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c38c84",
   "metadata": {},
   "source": [
    "### Fixing states\n",
    "#### los angeles (Ken Jee idea)\n",
    "#### my scraped data has more to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3146df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "df['job_state'] = df.job_state.apply(lambda x: x.strip() if x.strip().lower() != 'los angeles' else 'CA')\n",
    "df['job_state'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "df['job_state'] = df.job_state.apply(lambda x: x.strip() if x.strip().lower() != 'iowa' else 'IA')\n",
    "df['job_state'] = df.job_state.apply(lambda x: x.strip() if x.strip().lower() != 'georgia' else 'GA')\n",
    "df['job_state'] = df.job_state.apply(lambda x: x.strip() if x.strip().lower() != 'virginia' else 'VA')\n",
    "df['job_state'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96289ff",
   "metadata": {},
   "source": [
    "## Checking Job Description column (Ken Jee idea and +)\n",
    "### Counting lenght (Ken Jee idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17681d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#It might be interesting to know if companies have longer descriptions if they are posting higher or lower salaries.\n",
    "    #That is to say: Does a company want to explain away a low salary by having really in depth description???\n",
    "    #later we gonna compare to answer this hypothesis, For now, just a columns with these values\n",
    "\n",
    "df['desc_len'] = df['Job Description'].apply(lambda x:len(x))\n",
    "df['desc_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc9596",
   "metadata": {},
   "source": [
    "### I added a words counting column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "desc_wo_intro = df['Job Description']\n",
    "#First I replace '\\n' with ' ':\n",
    "for index, item in enumerate(desc_wo_intro):\n",
    "    if \"\\n\" in desc_wo_intro[index].lower():\n",
    "        desc_wo_intro[index] = desc_wo_intro[index].lower().replace('\\n',' ')\n",
    "# Then count words:\n",
    "df['desc_len_words'] = desc_wo_intro.str.count(' ') + 1\n",
    "df['desc_len_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e169158",
   "metadata": {},
   "source": [
    "## Competitors columns (Ken Jee idea)  \n",
    "#### This info is no longer provided  by glassdoor site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Competitors data are no longer provided by the glasdoor page, but I left the Ken Jee code here cause it represent an interesting research made by Ken Jee. Scondly, if someone wants to study Ken Jee extracted data (in 2020) can study this feature (I done. Do it yourself too).\n",
    "df['Competitors']\n",
    "#Here one can see that data has much blank rows referring to competitors (cell with -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_comp'] = df['Competitors'].apply(lambda x: len(x.split(',')) if x != '-1' else 0)\n",
    "df['num_comp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997f07d",
   "metadata": {},
   "source": [
    "## Re-Cleaning data (Ken Jee idea)\n",
    "### Hourly contemplation in salary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Well, I had done this in the Cleaning_Data phase, but I let the Ken Jee Code here (same reason last step).\n",
    "################################ hourly wage to annual ################################\n",
    "df['min_salary'] = df.apply(lambda x: x.min_salary*2 if x.hourly ==1 else x.min_salary, axis = 1)\n",
    "df['max_salary'] = df.apply(lambda x: x.max_salary*2 if x.hourly ==1 else x.max_salary, axis = 1)\n",
    "# Re-averaging step is still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e756ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I mean this line was missing:\n",
    "df['avg_salary'] = (int(df.min_salary) + int(df.max_salary))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0a6f0",
   "metadata": {},
   "source": [
    "## Checking Company Names column (Ken Jee idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#If we watch what we have in the company names trated column:\n",
    "df['company_txt']\n",
    "#It is quite evident that each cell has \"/n\" characters at the end. Then we shoul remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118dad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Ken Jee used this:\n",
    "#df['company_txt'] = df.company_txt.apply(lambda x: x.replace('\\n',''))\n",
    "#I think this is better:\n",
    "df['company_txt'] = df['company_txt'].str[:-1]\n",
    "df['company_txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb0246",
   "metadata": {},
   "source": [
    "## Checking the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfc661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically I repeat this WITH EACH COLUMN still not parsed:\n",
    "    # df['Rating']\n",
    "    # df['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebec409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Size Column\n",
    "df['Size'] = df['Size'].apply(lambda x: 'na' if 'unknown' in x.lower() else ('na' if '-1' in x.lower() else x))\n",
    "df['Size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "# Type of ownership \n",
    "df['Type of ownership'] = df['Type of ownership'].apply(lambda x: 'Other Organization' if 'unknown' in x.lower() else ('Other Organization' if '-1' in x.lower() else x))\n",
    "df['Type of ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "df['Revenue'] = df['Revenue'].apply(lambda x: 'Unknown / Non-Applicable' if '-1' in x.lower() else x)\n",
    "df['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6727f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Competitors\n",
    "    #NOTE: this information is no longer provided by the glassdoor site\n",
    "\n",
    "#df['Competitors']\n",
    "df['Competitors'].value_counts()\n",
    "#Could be interesting to check if infering that the company is trying to compete with bigger or smaller competitor to differentiate influence in the salary offered.\n",
    "#For this one should investigate in a different way (for emple in other site) the size of each competing company to compare.\n",
    "#Nevertheless, there are to much rows without this informations ('-1') so, it is a waste of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['hourly']\n",
    "#df['hourly'].value_counts()\n",
    "df[df.hourly == 1][['hourly','min_salary','max_salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['min_salary']\n",
    "#df['min_salary'].value_counts()\n",
    "\n",
    "#here I count how many rows are between the min_salary range I decided.\n",
    "#len_min_sal = len(df[(df['min_salary'] < 50)]) , len(df[(df['min_salary'] >= 50) & (df['min_salary'] < 60)]) , len(df[(df['min_salary'] >= 60) & (df['min_salary'] < 70)]) , len(df[(df['min_salary'] >= 70) & (df['min_salary'] < 80)]) , len(df[(df['min_salary'] >= 80) & (df['min_salary'] < 90)]) , len(df[(df['min_salary'] >= 90) & (df['min_salary'] < 100)]) , len(df[(df['min_salary'] >= 100) & (df['min_salary'] < 110)]) , len(df[(df['min_salary'] >= 110) & (df['min_salary'] < 120)]) , len(df[(df['min_salary'] >= 120) & (df['min_salary'] < 130)]) , len(df[(df['min_salary'] >= 130)])\n",
    "#len_min_sal\n",
    "\n",
    "#Here I take a look of some parts of the column\n",
    "#df[(df['min_salary'] < 50)][['min_salary']].value_counts()\n",
    "df[(df['min_salary'] > 130)][['min_salary']].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f229307",
   "metadata": {},
   "source": [
    "\n",
    "# \n",
    "# HARD EXPLORATION!!\n",
    "# \n",
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02772645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Ken Jee step\n",
    "#Ok, lets see what we done:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693095ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the same information avoiding '-1' cells:\n",
    "    #first, analyse each column.\n",
    "\n",
    "#df['Rating'].value_counts()\n",
    "#df[df['Rating']>0]['Rating'].value_counts()\n",
    "#len(df[(df['Rating'] > 0)])\n",
    "#df[(df['Rating'] > 0)].describe()\n",
    "dif_desc = df[(df['Rating'] > 0)]['Rating'].describe()\n",
    "\n",
    "#df['Founded'].value_counts()\n",
    "#df[df['Founded']>0]['Founded'].value_counts()\n",
    "#len(df[(df['Founded'] > 0)])\n",
    "#df[(df['Founded'] > 0)].describe()\n",
    "df[(df['Founded'] > 0)]['Founded'].describe()\n",
    "\n",
    "df['min_salary'].describe()\n",
    "\n",
    "df['max_salary'].describe()\n",
    "\n",
    "df['avg_salary'].describe()\n",
    "\n",
    "#df['age'].value_counts()\n",
    "#df[df['age']>0]['age'].value_counts()\n",
    "#len(df[(df['age'] > 0)])\n",
    "#df[(df['age'] > 0)].describe()\n",
    "df[(df['age'] > 0)]['age'].describe()\n",
    "\n",
    "#df['desc_len'].value_counts()\n",
    "#df[df['desc_len']>0]['desc_len'].value_counts()\n",
    "#len(df[(df['desc_len'] > 0)])\n",
    "#df[(df['desc_len'] > 0)].describe()\n",
    "df[(df['desc_len'] > 0)]['desc_len'].describe()\n",
    "\n",
    "#df['desc_len_words'].value_counts()\n",
    "#df[df['desc_len_words']>0]['desc_len_words'].value_counts()\n",
    "#len(df[(df['desc_len_words'] > 0)])\n",
    "#df[(df['desc_len_words'] > 0)].describe()\n",
    "df[(df['desc_len_words'] > 0)]['desc_len_words'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "    #Then, append the describe df columns:\n",
    "\n",
    "df_describe = df[(df['Rating'] > 0)][['Rating','hourly']].describe()\n",
    "df_describe['Founded'] = df[(df['Founded'] > 0)]['Founded'].describe()\n",
    "df_describe['min_salary'] = df['min_salary'].describe()\n",
    "df_describe['max_salary'] = df['max_salary'].describe()\n",
    "df_describe['avg_salary'] = df['avg_salary'].describe()\n",
    "df_describe['age'] = df[(df['age'] > 0)]['age'].describe()\n",
    "df_describe['desc_len'] = df[(df['desc_len'] > 0)]['desc_len'].describe()\n",
    "df_describe['desc_len_words'] = df[(df['desc_len_words'] > 0)]['desc_len_words'].describe()\n",
    "\n",
    "df_describe = df_describe.drop('hourly', axis = 'columns')\n",
    "\n",
    "df_describe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c403916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#df.Rating.hist() ##Ken Jee way\n",
    "#Watching Rating values without the -1 values.\n",
    "df[(df['Rating'] > 0)]['Rating'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2faf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Rating']>=0][['Rating']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa227fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Comparing average, min and max salary values without losing the sight in the individual graphics\n",
    "df.min_salary.hist()\n",
    "df.max_salary.hist()\n",
    "df.avg_salary.hist()\n",
    "\n",
    "df[['avg_salary','min_salary','max_salary']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column= ['min_salary','avg_salary','max_salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ebe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Here one of the graphics is enough, but I wonted to see both.\n",
    "df[(df['age'] > 0)&(df['Founded'] > 0)][['age','Founded']].hist(figsize=(15,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02702fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['age']>=0][['age']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48474224",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Watching the distribution in lenght of job description text characterized by characters and by words.\n",
    "df[['desc_len','desc_len_words']].hist(figsize=(15,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31539b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column= ['desc_len'])\n",
    "plt.show()\n",
    "df.boxplot(column= ['desc_len_words'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6001d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age','avg_salary','Rating','desc_len']].corr()\n",
    "#I think that when we gonna compare Ratings with other features the fact that it has -1 values is RELEVANT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "sns.heatmap(df[['age','avg_salary','Rating','desc_len']].corr(), vmax=.3,center=0,cmap=cmap,square=True,linewidths=.5,cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ad238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['age']>=0) & (df['Rating']>=0)][['age','avg_salary','min_salary','max_salary','Rating','desc_len','desc_len_words']].corr()\n",
    "    #I let min and max salary cause I would like to detect some correlation with a bigger salary range and a smaller rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "sns.heatmap(df[['age','avg_salary','min_salary','min_salary','Rating','desc_len']].corr(), vmax=.3,center=0,cmap=cmap,square=True,linewidths=.5,cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebbc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before, but without columns with '-1' values\n",
    "df[(df['age']>=0) & (df['Rating']>=0)][['age','avg_salary','Rating','desc_len','desc_len_words']].corr()\n",
    "    #This should not be more different than the previous, but more accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "sns.heatmap(df[(df['age']>=0) & (df['Rating']>=0)][['age','avg_salary','Rating','desc_len','desc_len_words']].corr(), vmax=.3,center=0,cmap=cmap,square=True,linewidths=.5,cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before, but without columns with '-1' values\n",
    "df[(df['age']>=0) & (df['Rating']>=0)][['avg_salary','desc_len','desc_len_words']].corr()\n",
    "    #This should not be more different than the previous, but more accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='avg_salary', y='desc_len_words')\n",
    "plt.show()\n",
    "df.plot.scatter(x='avg_salary', y='desc_len_words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c807b",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c49ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "#Comparing quantity of each keyword considered.\n",
    "#df.columns\n",
    "list_yn = ['python_yn', 'R_yn', 'spark_yn', 'aws_yn', 'excel_yn', 'sql_yn',\n",
    "       'sas_yn', 'd3js_yn', 'julia_yn', 'jupyter_yn', 'keras_yn', 'matlab_yn',\n",
    "       'matplotlib_yn', 'pytorch_yn', 'scikit_yn', 'tensor_yn', 'weka_yn',\n",
    "       'selenium_yn', 'hadoop_yn', 'tableau_yn', 'bi_yn', 'bigml_yn',\n",
    "       'rapidminer_yn', 'flink_yn', 'datarobot_yn', 'hana_yn', 'mongo_yn',\n",
    "       'trifacta_yn', 'minitab_yn', 'kafka_yn', 'microstrategy_yn',\n",
    "       'google_an_yn', 'spss_yn']\n",
    "#df[['python_yn','R_yn','spark_yn','aws_yn', 'excel_yn']].hist(bins=2,figsize=(10,10))\n",
    "df[list_yn].hist(bins=2,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b273ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER###############################################################################################################################\n",
    "df[list_yn].apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[list_yn].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0e83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "sns.heatmap(df[list_yn].corr(), vmax=.3,center=0,cmap=cmap,square=True,linewidths=.5,cbar_kws={\"shrink\": .5})\n",
    "#here can be seen wich words group more aften."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b50dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_yn_salary = ['avg_salary']\n",
    "list_yn_salary.extend(list_yn)\n",
    "\n",
    "df[list_yn_salary].corr()\n",
    "#Aplying point biserial correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e181ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "sns.heatmap(df[list_yn_salary].corr(), vmax=.7,center=0,cmap=cmap,square=True,linewidths=.5,cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3dd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I take only the rows with the correspondent keyword each time\n",
    "    #I mean, For example, I read the python_yn column and take just the rows where I have '1' (those who say 'python' in the job description)\n",
    "    #then with those rows (in a new df) I apply function .describe()\n",
    "    #And star again with each keyword.\n",
    "    #Note: first column is calculated with all the rows in the original cleaned df.\n",
    "def kw_to_avg_salary(dff, list_yn):\n",
    "    df_listyn = dff[['avg_salary']].describe()\n",
    "    for index, item in enumerate(list_yn):\n",
    "        df_listyn['av_sal_'+list_yn[index]] = dff[dff[list_yn[index]]>0]['avg_salary'].describe()\n",
    "    return df_listyn   \n",
    "        \n",
    "        \n",
    "desc_kw_to_avg_salary = kw_to_avg_salary(df,list_yn)\n",
    "desc_kw_to_avg_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5b838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "# sns.color_palette(\"mako\", as_cmap=True)\n",
    "sns.heatmap(desc_kw_to_avg_salary, vmin=40, vmax=240,center=120,cmap=\"cubehelix\",square=True,linewidths=.5,cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260833dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_kw_to_avg_salary.loc[['mean']]\n",
    "#Another way to do this:    #df[list_yn_salary].vivot()\n",
    "                            #pd.pivot_table(df, index = 'job_simp', values = 'avg_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea4261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "# sns.color_palette(\"mako\", as_cmap=True)\n",
    "sns.heatmap(desc_kw_to_avg_salary.loc[['mean']], vmin=80, vmax=120,center=99,cmap=\"rocket_r\",square=True,linewidths=.5,cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968d7f6",
   "metadata": {},
   "source": [
    "## Anlyzing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, I want to compare how many times the keywords are labeled in each Job Description with the salaries. This can be considered as \"while more especific the skill descripted is the salary better?\"\n",
    "    #firs I creat a new column that count how many words are labeled in each Job Description.\n",
    "df['count_keywords'] = df[list_yn].sum(axis=1)\n",
    "df['count_keywords'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d994d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_keywords'].hist()\n",
    "#df['count_keywords'].plot(kind='bar',rot=90,figsize=[15,15])\n",
    "plt.show()\n",
    "df.boxplot(column= ['count_keywords'])\n",
    "plt.show()\n",
    "df_list_yn=df[list_yn].apply(pd.value_counts)\n",
    "df_list_yn[1:].plot(kind='bar',rot=90,figsize=[10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd41a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['python_yn']>0].plot.scatter(x='count_keywords', y='avg_salary')\n",
    "#At first, no correlation can be seen. (Naturally, we are working with a categorical variable and a numerical one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818cee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['avg_salary','python_yn']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6644dd2",
   "metadata": {},
   "source": [
    "## Comparing lenght descriptions, quantity of keywords and salaries columns:\n",
    "## -PROBABILITY -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['min_salary','max_salary','avg_salary','count_keywords','desc_len']].corr(method ='pearson')\n",
    "#We can see that a lineal correlation is not a right choice to find a grade of dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25bc05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[['min_salary','max_salary','avg_salary','count_keywords','desc_len']].corr(method ='kendall')\n",
    "#Even worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd83b5c",
   "metadata": {},
   "source": [
    "We can take into acount that the correlation is calculated based in lineal geometry.\n",
    "\n",
    "But, what happen if we cluster values by quantiles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe56fd",
   "metadata": {},
   "source": [
    "### Quantity of keywords mentioned and  Job Descriptions lenghts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e031268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the number of keywords equivalent to the lenght of the description?\n",
    "    #In other words, if the lenght of the description is from the 3rd quantile, so is the number of keywords\n",
    "    #This means that in this rows I'm talking about a job whose descriptions is between the longest and have the more mentioned keywords.\n",
    "    \n",
    "    \n",
    "def kw_dl_same_quantile(dff,list_2_col,kw_dl_quant): #Comparing row to row - quantile to quantile\n",
    "    if dff[list_2_col[0]]>kw_dl_quant.at[0.75,list_2_col[0]] and dff[list_2_col[1]]>kw_dl_quant.at[0.75,list_2_col[1]]:\n",
    "        return 1\n",
    "    elif dff[list_2_col[0]]>kw_dl_quant.at[0.5,list_2_col[0]] and dff[list_2_col[0]]<=kw_dl_quant.at[0.75,list_2_col[0]] and dff[list_2_col[1]]>kw_dl_quant.at[0.5,list_2_col[1]] and dff[list_2_col[1]]<=kw_dl_quant.at[0.75,list_2_col[1]]:\n",
    "        return 1\n",
    "    elif dff[list_2_col[0]]>kw_dl_quant.at[0.25,list_2_col[0]] and dff[list_2_col[0]]<=kw_dl_quant.at[0.5,list_2_col[0]] and dff[list_2_col[1]]>kw_dl_quant.at[0.25,list_2_col[1]] and dff[list_2_col[1]]<=kw_dl_quant.at[0.5,list_2_col[1]]:\n",
    "        return 1\n",
    "    elif dff[list_2_col[0]]<kw_dl_quant.at[0.25,list_2_col[0]] and dff[list_2_col[1]]<kw_dl_quant.at[0.25,list_2_col[1]]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compare_2columns_rbr_quant(df_entry, list_two_col):\n",
    "    #First I define quantiles values\n",
    "    kw_dl_quant = df[list_two_col].quantile([.25,.5,.75,1])\n",
    "    \n",
    "    #and run the previous function row to row.\n",
    "    nkw_eq_nld = df_entry[list_two_col].apply(lambda x: kw_dl_same_quantile(x,list_two_col,kw_dl_quant),axis=1)\n",
    "        \n",
    "    #cCounting how many rows meet the conditions.\n",
    "    nkw_eq_nld.value_counts()\n",
    "        #Note: A low quantity of equivalents means that there is no correlation between the quantity of the mentioned keywords and the lenght of the description\n",
    "\n",
    "    #returning the probability to meet conditions (both colummns to the same percentile).\n",
    "    return nkw_eq_nld.value_counts().at[1]/len(nkw_eq_nld)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dddb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing quantity of keywords with job descriptions lenght, row to row, quantile to quantile\n",
    "    #\"is the quantile in keywords quantity the same as te \"\n",
    "list_two_col = ['count_keywords','desc_len'] #Here what I whant to compare line by line\n",
    "compare_2columns_rbr_quant(df,list_two_col)\n",
    "\n",
    "#Here we take that just the 24% of the rows meet the conditions. This says that there is no direct correlation between both characteristics (lenght of job descriptions and quantity of keywords) or a poor one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3acabd",
   "metadata": {},
   "source": [
    "### SALARY (average, min and max) and Job Descriptions lenght "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_two_col = ['avg_salary','desc_len'] #Here what I whant to compare line by line\n",
    "compare_2columns_rbr_quant(df,list_two_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_two_col = ['min_salary','desc_len'] #Here what I whant to compare line by line\n",
    "compare_2columns_rbr_quant(df,list_two_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_two_col = ['max_salary','desc_len'] #Here what I whant to compare line by line\n",
    "compare_2columns_rbr_quant(df,list_two_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1415f9b",
   "metadata": {},
   "source": [
    "### Quantity of keywords mentioned and SALARY (average, min and max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0520fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_two_col = ['count_keywords','avg_salary'] #Here what I whant to compare line by line\n",
    "compare_2columns_rbr_quant(df,list_two_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15108d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_two_col = ['count_keywords','min_salary'] #Here what I whant to compare line by line\n",
    "compare_2columns_rbr_quant(df,list_two_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_two_col = ['count_keywords','max_salary'] #Here what I whant to compare line by line\n",
    "compare_2columns_rbr_quant(df,list_two_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515cfd21",
   "metadata": {},
   "source": [
    "# ME QUEDÉ ACÁ\n",
    "# ############################################################\n",
    "# ############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d95b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175007aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f5a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871611a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e3356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e10c2744",
   "metadata": {},
   "source": [
    "## Just and only focus on Categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86583b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat = df[[ 'Location', 'Headquarters','Size', 'Type of ownership', 'Industry', 'Sector', 'Revenue','company_txt','job_state','same_state','R_yn',\n",
    "#        'spark_yn', 'aws_yn', 'excel_yn', 'py_yn', 'job_simp', 'seniority']]\n",
    "\n",
    "df_cat = df[[ 'Location', 'Headquarters','Size', 'Type of ownership', 'Industry', 'Sector', 'Revenue','company_txt','job_state','job_simp', 'seniority']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a49ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in df_cat.columns:\n",
    "    cat_num=df_cat[i].value_counts()\n",
    "    print(\"graph for %s: total = %d\" % (i, len(cat_num)))\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    chart = sns.barplot(x=cat_num.index,y=cat_num)\n",
    "    #label rotation\n",
    "    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8f10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i in df_cat[['Location', 'Headquarters','company_txt']].columns:\n",
    "for i in ['Location', 'Headquarters','company_txt']:    \n",
    "    #this time, just the top 20 ([:20])\n",
    "    cat_num=df_cat[i].value_counts()[:20]\n",
    "    print(\"graph for %s: total = %d\" % (i, len(cat_num)))\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    chart = sns.barplot(x=cat_num.index,y=cat_num)\n",
    "    #label rotation\n",
    "    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67405fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index = 'job_simp', values = 'avg_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144eabc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index = ['job_simp','seniority'], values = 'avg_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff857a76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index = ['job_simp','seniority','python_yn'], values = 'avg_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0138b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index = 'job_state', values = 'avg_salary').sort_values('avg_salary',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6841940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  \n",
    "pd.pivot_table(df, index = ['job_simp','job_state'], values = 'avg_salary').sort_values(['job_simp','avg_salary'],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728274d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st_js_sal=pd.pivot_table(df, index = ['job_state','job_simp'], values = 'avg_salary').sort_values(['job_state','job_simp'],ascending=False)\n",
    "st_js_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742103e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)                 #This is to see the whole list.\n",
    "st_js_count = pd.pivot_table(df, index = ['job_state','job_simp'], values = 'avg_salary', aggfunc='count').sort_values(['job_state','job_simp'],ascending=False)\n",
    "st_js_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3cd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_js_sal['count'] = st_js_count \n",
    "st_js_sal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1b33e",
   "metadata": {},
   "source": [
    "# The previous step is wrong for Ken Jee.\n",
    "# He doesnt ordered de list and doesn't saw the complete quantity of jobs in NY\n",
    "### https://youtu.be/QWgg4w1SpJ8?t=2759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96254f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df[df.job_simp == 'data scientist'], index = 'job_state', values = 'avg_salary').sort_values('avg_salary',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d26052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating, industry, sector, revenue, number of comp, hourly, employer provided, desc_len, Type of ownership\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07622f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pivots = ['Rating','Industry', 'Sector', 'Revenue','hourly','employer_provided','Type of ownership','avg_salary']\n",
    "#df_pivots = df[['Rating','Industry', 'Sector', 'Revenue','hourly','employer_provided','Type of ownership','avg_salary']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_pivots[:-1]:\n",
    "    print(i)\n",
    "    print(pd.pivot_table(df[list_pivots], index=i,values='avg_salary').sort_values('avg_salary',ascending=False))\n",
    "    \n",
    "# for i in df_pivots.columns[:-1]:\n",
    "#     print(i)\n",
    "#     print(pd.pivot_table(df_pivots, index=i,values='avg_salary').sort_values('avg_salary',ascending=False))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582e1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rev_yn = pd.pivot_table(df, index = 'Revenue',columns = 'python_yn',values='avg_salary',aggfunc='count').sort_values('Revenue',ascending=True)\n",
    "for i in list_yn:#[-1:]:\n",
    "    rev_yn_ = pd.pivot_table(df, index = 'Revenue',columns = i,values='avg_salary',aggfunc='count').sort_values('Revenue',ascending=True)\n",
    "    if 1 in rev_yn_.columns:\n",
    "        rev_yn[i] = rev_yn_[1]\n",
    "rev_yn\n",
    "\n",
    "#pd.pivot_table(df, index = 'Revenue',columns = 'python_yn',values='avg_salary',aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451af010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77164df",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \" \".join(df['Job Description'])\n",
    "\n",
    "def punctuation_stop(text):\n",
    "    \"\"\"remove punctuation and stop words\"\"\"\n",
    "    filtered = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words and w.isalpha():\n",
    "            filtered.append(w.lower())\n",
    "    return filtered\n",
    "\n",
    "\n",
    "words_filtered = punctuation_stop(words)\n",
    "\n",
    "text = \" \".join([ele for ele in words_filtered])\n",
    "\n",
    "wc= WordCloud(background_color=\"white\", random_state=1,stopwords=STOPWORDS, max_words = 2000, width =800, height = 1500)\n",
    "wc.generate(text)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e9438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
